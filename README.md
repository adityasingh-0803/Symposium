# Symposium: Talking to Virtual Minds

## ğŸ§  Project Overview

**Symposium: Talking to Virtual Minds** is an AI-powered, emotion-aware conversational experience featuring embodied avatars of visionary leaders across domains like space, design, and medicine. Users engage in **one-on-one conversations**, where each AI persona responds dynamically to the user's selected emotion â€” complete with expressive speech, lip-sync animation, and intelligent background transitions.

Built using a fusion of **Conversational AI**, **Text-to-Speech**, **Generative Visuals**, and **Unity avatars**, Symposium reimagines how we interact with intelligent systems through immersive storytelling.

---

## ğŸ¥ Project Demo

ğŸ”— **[Watch the Demo Video (60â€“90s)](https://drive.google.com/file/d/1253jno2u23covNzBAOvY9_V8qYdhKO1m/view?usp=sharing)**  
ğŸ•¹ï¸ A complete walkthrough of the immersive experience, from emotion selection to avatar response and real-time background changes.

> *(Replace the above link with your actual hosted video â€” YouTube, Drive, or embedded player.)*

---

## ğŸ¯ Key Features

- ğŸ­ Emotion-Driven Dialogue with GPT
- ğŸ‘¤ Embodied AI Leaders (Cooper, Elena Sparks, Dr. Aanya Verma)
- ğŸ§ Realistic Voice via ElevenLabs
- ğŸŒŒ Dynamic AI Backgrounds triggered by topic
- ğŸ§  Full Conversational Pipeline from emotion â†’ avatar response

---

## ğŸ§© Technology Stack

| Component             | Tools / Models Used                          |
|-----------------------|----------------------------------------------|
| Natural Language Gen  | GPT-4 / Claude                               |
| Text-to-Speech (TTS)  | ElevenLabs / Replica Studios                 |
| Image Generation      | Midjourney / DALLÂ·E                          |
| Avatar Creation       | ReadyPlayerMe / Mixamo                       |
| Lip Sync              | SALSA LipSync / Oculus LipSync              |
| Background Switching  | Unity Timeline + GPT theme segmentation     |
| Audio Mixing          | Unity Audio Mixer / Audacity                |

---

## ğŸ‘¤ AI Leaders

| Name               | Company            | Field         | Tagline                             |
|--------------------|--------------------|---------------|--------------------------------------|
| **Cooper**         | AstroCore          | Space AI      | "Guiding humanity beyond stars."     |
| **Elena Sparks**   | DesigNext AI       | Creative AI   | "Design for everyone, intelligently."|
| **Dr. Aanya Verma**| NeuroPulse AI      | Medical AI    | "Healing at the speed of thought."   |

---

## âš™ï¸ How It Works

1. **User selects an emotion** (e.g., hopeful, sad, curious).
2. GPT generates an **emotion-matched speech** for a chosen AI leader.
3. ElevenLabs produces the **spoken voice** with tone control.
4. Unity avatar plays the voice, **synchronized with lip movement**.
5. Backgrounds change dynamically, powered by GPT-extracted themes.

---


---

## ğŸš€ Future Work

- Real-time input via Whisper + GPT + TTS pipeline
- Emotion detection via facial expression / voice tone
- Multi-agent group conversation mode
- Memory-based persistent avatars
- Explainable AI dialogue (ethical alignment questions)

---

## ğŸ‘¥ Team & Roles

- **Conversational AI & Prompts** â€“ [Your Name]
- **Unity & Scene Integration** â€“ [Teammate Name]
- **Voice & Audio Mixing** â€“ [Teammate Name]
- **Avatar Design & Lip Sync** â€“ [Teammate Name]

---

## ğŸ“ Submission Info

- **Challenge:** AI Video Generation Challenge â€“ Visionary Leaders Edition  
- **Project Title:** *Symposium: Talking to Virtual Minds*  
- **Duration:** 60â€“90 seconds  
- **Category:** AI + Conversational Interfaces + Embodied Agents  

---

## ğŸ“¬ Contact
Team name: SimLink


